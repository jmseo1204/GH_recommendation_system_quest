{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 본 실습에서 사용할 데이터셋은 deepFM에서 사용한 데이터셋과 동일합니다!\n",
    "# 단순히 벤치마크 데이터셋 그대로를 사용하기보다, 우리가 가지고있는 데이터셋을 모델 규격에 맞게 가공하여 입력으로 넣어봅시다.\n",
    "# 1) SASRec은 시계열(시퀀스) 모델로, 데이터셋에서 각 유저(ip) 별 시청한 아이템(app)이 시간 순으로 정렬되어있음을 가정합니다.\n",
    "# 2) 또한 모델 학습 및 추론 과정에서 사용되는 정보는 유저id와 아이템id가 유일합니다.\n",
    "# 3) 마지막으로, SASRec은 특정 유저의 과거 선택 기록을 기반으로 현재 어떤 아이템을 선택할지 예측하는 모델이라는 점에서 양성(label=True) 데이터만 남기고 음성 데이터는 전부 지워줍니다.\n",
    "# 위 3단계를 구현해봅시다.\n",
    "\n",
    "df = pd.read_csv('./train_sample.csv')\n",
    "\n",
    "# 양성 데이터만 선별\n",
    "df = df.loc[df['is_attributed']==1]\n",
    "\n",
    "# 유저 id와 아이템 id는 1부터 +1씩 증가하도록 변환해야합니다. 왜 이렇게 불편하게 설계했는지는 잘 모르겠지만.. 그걸 가정하니 규격에 맞게 바꿔줍시다.\n",
    "for feat in ['ip', 'app']: \n",
    "    le = LabelEncoder() \n",
    "    df[feat] = le.fit_transform(df[feat])+1\n",
    "    \n",
    "# 유저 별, 시간 순 정렬\n",
    "df = df.sort_values(by=['ip','click_time'])\n",
    "\n",
    "# 필요한 정보(유저id, 아이템id)만 .txt(sep는 ','이 아니라 ' ' 입니다.)으로 저장\n",
    "df[['ip', 'app']].to_csv('./data/movie.txt', sep=' ', header=False, index=False)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sequence length: 1.62\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from model import SASRec\n",
    "from utils import *\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "\n",
    "\n",
    "def str2bool(s):\n",
    "    if s not in {\"false\", \"true\"}:\n",
    "        raise ValueError(\"Not a valid boolean string\")\n",
    "    return s == \"true\"\n",
    "\n",
    "\n",
    "\n",
    "conf_url = 'hyperparameters.yaml'\n",
    "with open(conf_url, 'r') as f:\n",
    "\tconfig_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "args = Box(config_yaml)\n",
    "\n",
    "\n",
    "\n",
    "args.dataset = 'movie' ####### TODO: data 폴더 내부 txt파일명(suffix 제외) ex) ml-1m, Beauty 등. 우리는 movie.txt로 저장했으니 movie를 적어줍시다.\n",
    "args.train_dir = 'test' ####### TODO: 모델 저장할 폴더명 아무거나 (앞으로 ./{dataset}_{train_dir} 폴더에 log, model 저장됨)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.isdir(args.dataset + \"_\" + args.train_dir):\n",
    "    os.makedirs(args.dataset + \"_\" + args.train_dir)\n",
    "with open(os.path.join(args.dataset + \"_\" + args.train_dir, \"args.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join([str(k) + \",\" + str(v) for k, v in sorted(vars(args).items(), key=lambda x: x[0])]))\n",
    "f.close()\n",
    "\n",
    "u2i_index, i2u_index = build_index(args.dataset)\n",
    "\n",
    "# global dataset\n",
    "dataset = data_partition(args.dataset)\n",
    "\n",
    "[user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
    "# num_batch = len(user_train) // args.batch_size # tail? + ((len(user_train) % args.batch_size) != 0)\n",
    "num_batch = (len(user_train) - 1) // args.batch_size + 1\n",
    "cc = 0.0\n",
    "for u in user_train:\n",
    "    cc += len(user_train[u])\n",
    "print(\"average sequence length: %.2f\" % (cc / len(user_train)))\n",
    "\n",
    "f = open(os.path.join(args.dataset + \"_\" + args.train_dir, \"log.txt\"), \"w\")\n",
    "f.write(\"epoch (val_ndcg, val_hr) (test_ndcg, test_hr)\\n\")\n",
    "\n",
    "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=3)\n",
    "model = SASRec(usernum, itemnum, args).to(args.device)  # no ReLU activation in original SASRec implementation?\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    try:\n",
    "        torch.nn.init.xavier_normal_(param.data)\n",
    "    except:\n",
    "        pass  # just ignore those failed init layers\n",
    "\n",
    "model.pos_emb.weight.data[0, :] = 0\n",
    "model.item_emb.weight.data[0, :] = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 1 iteration 0: 1.271963119506836\n",
      "loss in epoch 1 iteration 191: 0.3266589045524597\n",
      "loss in epoch 1 iteration 382: 0.35938864946365356\n",
      "loss in epoch 1 iteration 573: 0.3367638885974884\n",
      "loss in epoch 1 iteration 764: 0.32608458399772644\n",
      "loss in epoch 1 iteration 955: 0.31127694249153137\n",
      "loss in epoch 1 iteration 1146: 0.3039439022541046\n",
      "loss in epoch 1 iteration 1337: 0.387241393327713\n",
      "loss in epoch 1 iteration 1528: 0.26362931728363037\n",
      "loss in epoch 1 iteration 1719: 0.300498366355896\n",
      "loss in epoch 1 iteration 1910: 0.3208335041999817\n",
      "Evaluating......................epoch:1, time: 32.614681(s), valid (NDCG@10: 0.8462, HR@10: 0.9563), test (NDCG@10: 0.8406, HR@10: 0.9662)\n",
      "loss in epoch 2 iteration 0: 0.2981089949607849\n",
      "loss in epoch 2 iteration 191: 0.34493815898895264\n",
      "loss in epoch 2 iteration 382: 0.332329660654068\n",
      "loss in epoch 2 iteration 573: 0.29634028673171997\n",
      "loss in epoch 2 iteration 764: 0.23454266786575317\n",
      "loss in epoch 2 iteration 955: 0.2861711382865906\n",
      "loss in epoch 2 iteration 1146: 0.33344680070877075\n",
      "loss in epoch 2 iteration 1337: 0.3485826849937439\n",
      "loss in epoch 2 iteration 1528: 0.37942183017730713\n",
      "loss in epoch 2 iteration 1719: 0.29508674144744873\n",
      "loss in epoch 2 iteration 1910: 0.24670270085334778\n",
      "Evaluating.......................epoch:2, time: 64.376449(s), valid (NDCG@10: 0.8482, HR@10: 0.9528), test (NDCG@10: 0.8522, HR@10: 0.9646)\n",
      "loss in epoch 3 iteration 0: 0.2649536728858948\n",
      "loss in epoch 3 iteration 191: 0.3717968463897705\n",
      "loss in epoch 3 iteration 382: 0.2930150032043457\n",
      "loss in epoch 3 iteration 573: 0.31350842118263245\n",
      "loss in epoch 3 iteration 764: 0.3924575448036194\n",
      "loss in epoch 3 iteration 955: 0.27817240357398987\n",
      "loss in epoch 3 iteration 1146: 0.305730402469635\n",
      "loss in epoch 3 iteration 1337: 0.3002918064594269\n",
      "loss in epoch 3 iteration 1528: 0.2853412330150604\n",
      "loss in epoch 3 iteration 1719: 0.3581392765045166\n",
      "loss in epoch 3 iteration 1910: 0.27754339575767517\n",
      "Evaluating........................epoch:3, time: 98.136102(s), valid (NDCG@10: 0.8639, HR@10: 0.9631), test (NDCG@10: 0.8450, HR@10: 0.9478)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()  # enable model training\n",
    "\n",
    "epoch_start_idx = 1\n",
    "\n",
    "# ce_criterion = torch.nn.CrossEntropyLoss()\n",
    "# https://github.com/NVIDIA/pix2pixHD/issues/9 how could an old bug appear again...\n",
    "bce_criterion = torch.nn.BCEWithLogitsLoss()  # torch.nn.BCELoss()\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))\n",
    "\n",
    "best_val_ndcg, best_val_hr = 0.0, 0.0\n",
    "best_test_ndcg, best_test_hr = 0.0, 0.0\n",
    "T = 0.0\n",
    "t0 = time.time()\n",
    "fname = f\"SASRec_epoch{args.num_epochs}.pth\"\n",
    "folder = args.dataset + \"_\" + args.train_dir\n",
    "save_path = \"\"\n",
    "\n",
    "\n",
    "for epoch in range(epoch_start_idx, args.num_epochs + 1):\n",
    "    for step in range(num_batch):  # tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):\n",
    "\n",
    "        u, seq, pos, neg = sampler.next_batch()  # tuples to ndarray\n",
    "        u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
    "        pos_logits, neg_logits = model(u, seq, pos, neg)\n",
    "        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(\n",
    "            neg_logits.shape, device=args.device\n",
    "        )\n",
    "        # print(\"\\neye ball check raw_logits:\"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0\n",
    "        adam_optimizer.zero_grad()\n",
    "        indices = np.where(pos != 0)\n",
    "        loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n",
    "        loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n",
    "        for param in model.item_emb.parameters():\n",
    "            loss += args.l2_emb * torch.norm(param)\n",
    "        loss.backward()\n",
    "        adam_optimizer.step()\n",
    "        \n",
    "        if step%(num_batch//10) == 0:\n",
    "            print(\n",
    "                \"loss in epoch {} iteration {}: {}\".format(epoch, step, loss.item())\n",
    "            )  # expected 0.4~0.6 after init few epochs\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "        t1 = time.time() - t0\n",
    "        T += t1\n",
    "        print(\"Evaluating\", end=\"\")\n",
    "        t_test = evaluate(model, dataset, args)\n",
    "        t_valid = evaluate_valid(model, dataset, args)\n",
    "        print(\n",
    "            \"epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)\"\n",
    "            % (epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1])\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            t_valid[0] > best_val_ndcg\n",
    "            or t_valid[1] > best_val_hr\n",
    "            or t_test[0] > best_test_ndcg\n",
    "            or t_test[1] > best_test_hr\n",
    "        ):\n",
    "            best_val_ndcg = max(t_valid[0], best_val_ndcg)\n",
    "            best_val_hr = max(t_valid[1], best_val_hr)\n",
    "            best_test_ndcg = max(t_test[0], best_test_ndcg)\n",
    "            best_test_hr = max(t_test[1], best_test_hr)\n",
    "            folder = args.dataset + \"_\" + args.train_dir\n",
    "            temp_fname = f\"SASRec_epoch{epoch}.pth\"\n",
    "            save_path = os.path.join(folder, temp_fname)\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        f.write(str(epoch) + \" \" + str(t_valid) + \" \" + str(t_test) + \"\\n\")\n",
    "        f.flush()\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "\n",
    "    if epoch == args.num_epochs:\n",
    "        save_path = os.path.join(folder, fname)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "f.close()\n",
    "sampler.close()\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15732\\587850054.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(args.state_dict_path, map_location=torch.device(args.device)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............test (NDCG@10: 0.8442, HR@10: 0.9521)\n"
     ]
    }
   ],
   "source": [
    "args.state_dict_path = save_path\n",
    "model = SASRec(usernum, itemnum, args).to(args.device)  # no ReLU activation in original SASRec implementation?\n",
    "model.load_state_dict(torch.load(args.state_dict_path, map_location=torch.device(args.device)))\n",
    "\n",
    "model.eval()\n",
    "t_test = evaluate(model, dataset, args)\n",
    "print(\"test (NDCG@10: %.4f, HR@10: %.4f)\" % (t_test[0], t_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Quest]\n",
    "\n",
    "1) squence model의 train_test_split method\n",
    "\n",
    "train 단계와 test 단계에서 데이터를 어떻게 구분하는지 고려해봐야합니다.\n",
    "\n",
    "만약 100명의 유저가 있다고 하면, 80:20으로 구분하여 train_test_split을 했을까요?\n",
    "그렇다면 train단계에서 학습되지 못한 20명의 유저 정보를 추론할 때 정확도가 높지 않을 확률이 높습니다.\n",
    "해당 모델은 시퀀스 모델이기에 일반적인 피처 예측 모델과는 꽤 다른 방식으로 train, valid, test 데이터를 구분합니다. \n",
    "소스코드를 살펴보면서 어떤 방식으로 train_test_split을 하는지 기술해봅시다. 왜 해당 방식으로 split하는 게 합당한지 본인의 생각을 적어주세요.\n",
    "\n",
    "(힌트: util.py의 data_partition, evaluate 함수를 살펴봅시다. torch, numpy를 잘 몰라 이해가 잘 안된다면 주석을 참고해서 작동 방식을 추측해주셔도 좋습니다!)\n",
    "\n",
    "답변: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) NDCG, HR 성능평가지표의 의미\n",
    "\n",
    " 본 코드에서는 SASRec 모델의 성능 지표로 NDCG@10, HR@10을 사용하고 있습니다. \n",
    "작성자의 경우, NDCG@10은 0.84, HR@10은 0.95 정도가 나오는데요. \n",
    "\n",
    "\n",
    "2-1) HR@10이 0.95가 나온다는 게 무슨 의미인지 HR@10의 정의를 고려하여 설명해봅시다.\n",
    "\n",
    "답변: \n",
    "\n",
    "\n",
    "2-2) NDCG@10=0.88, HR@10=0.9 가 나왔다고 가정해봅시다. 이 두 지표가 유사한 값을 가진다는 게 무슨 의미인지 NDCG@10의 정의를 고려하여 설명해봅시다.\n",
    "\n",
    "답변:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) deepFM과 SASRec의 차이\n",
    "\n",
    "deepFM, SASRec 두 가지 딥러닝 기반 추천시스템 모델을 사용해보았습니다. \n",
    "과연 두 모델 중 어떤 모델이 특정 상황에서 더 적합할지 생각해봐야 합니다.\n",
    "\n",
    "다음 상황을 가정해봅시다. 그핵이는 이커머스 플랫폼 GH팡에서 소비자들에게 상품을 추천해주는 추천시스템을 만들고 있습니다.\n",
    "그러나 그핵이가 마주한 문제점은 대부분의 소비자들은 과거 구매 데이터가 거의 없어서 무엇을 좋아할지 예측하기 힘들다는 점이었습니다.\n",
    "해당 비활성 고객으로부터 알아낼 수 있는 유일한 정보는 연령, 성별, 관심카테고리 등 서비스 회원가입을 할 때 필수적으로 기입해야하는 정보 뿐입니다.\n",
    "이렇게 활성 고객과 비활성 고객이 섞여있는 유저 집단에 맞춤형 추천을 하기 위해서 deepFM과 SASRec을 어떻게 활용하면 좋을지 본인의 생각에 따라 추천시스템 구조를 간단하게 설계하고 그 근거를 말해봅시다.\n",
    "\n",
    "(생각해볼 포인트: 활성 정도에 따라 고객을 클러스터링하는 데 성공했다면, 클러스터 별로 다른 모델을 적용할 수도 있습니다. 어떤 군집 패턴을 분석할 때 어떤 모델에 더 적합할지 생각해봅시다.)\n",
    "\n",
    "답변:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
